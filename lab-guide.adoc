[[containerization-and-kubernetes-lab--aws-learner-lab]]
== Containerization and Kubernetes Lab — AWS Learner Lab

=== Lab Overview

This lab takes you from building your first container image to running a
multi-service application on a production Kubernetes cluster. You will
containerize a Python web application and a background worker, push the
images to Amazon ECR, deploy them to an Amazon EKS cluster, and then
observe how Kubernetes handles scaling, self-healing, rolling updates,
and failure recovery.

The application is a distributed text-processing pipeline. A Flask web
app accepts text through HTTP and queues it in Redis. Worker containers
pull text from the queue, compute word-frequency analysis, and store
results back in Redis. This is a producer-consumer pattern — the same
architecture used in large-scale data processing systems — but here
Kubernetes manages the workers rather than you provisioning and
monitoring them manually.

*Estimated time:* 3–4 hours across two sessions +
*AWS budget impact:* ~$0.60–0.80 per session (see teardown instructions
to avoid ongoing charges)

'''''

=== How This Lab Handles Sessions

The Learner Lab has session timers. When a session expires, EC2
instances (including your EKS nodes) are stopped. When you start a new
session, they restart automatically. Your EKS cluster, ECR images, and
Kubernetes resource definitions all persist.

The lab is structured around two natural stopping points:

*Session A* covers infrastructure setup and containerization — creating
the EKS cluster, building images, and pushing them to ECR. You can stop
here and resume days later.

*Session B* covers deploying the application to Kubernetes and running
experiments. This is where the orchestration concepts come alive.

*Resuming a session:* Open CloudShell, run `kubectl get nodes`, and
verify your nodes are in `Ready` status. If they show `NotReady`, wait
2–3 minutes for the nodes to finish restarting. Everything else is
already in place.

'''''

=== Session A: Infrastructure and Containerization

[[a1--set-up-your-cloudshell-workspace-10-minutes]]
==== A1 — Set Up Your CloudShell Workspace (10 minutes)

CloudShell is your primary terminal for this lab. It has AWS credentials
pre-configured and a 1 GB home directory that persists between sessions.

Open CloudShell from the AWS Console (the terminal icon in the top
navigation bar). Then install the tools you'll need:

[source,bash]
----
# Clone the lab repository
cd ~
git clone <REPO_URL> k8s-lab
cd k8s-lab

# Install kubectl and eksctl
chmod +x scripts/*.sh
./scripts/setup-tools.sh
----

Verify the installations:

[source,bash]
----
kubectl version --client --short 2>/dev/null || kubectl version --client
eksctl version
----

Both commands should print version numbers without errors. These tools
persist in your CloudShell home directory across sessions.

[[a2--create-the-eks-cluster-20-minutes-mostly-waiting]]
==== A2 — Create the EKS Cluster (20 minutes, mostly waiting)

This step creates a Kubernetes cluster with two worker nodes. Cluster
creation takes 12–18 minutes. You'll use the waiting time productively
in the next step.

First, the script needs to discover the IAM roles that the Learner Lab
pre-provisioned for EKS. Run the creation script:

[source,bash]
----
./scripts/create-cluster.sh
----

The script will:

[arabic]
. Find the EKS IAM roles in your account
. Discover your default VPC and subnets
. Create an EKS cluster
. Create a managed node group with two `t3.small` instances
. Configure `kubectl` to connect to the cluster

You'll see output as each step completes. The cluster creation step
prints a message telling you to continue with the next section while it
provisions.

*What's happening behind the scenes:* EKS is spinning up the Kubernetes
control plane — the API server, etcd database, scheduler, and controller
manager — as a managed service. You don't see these components directly
(unlike a self-managed cluster), but they're running. The node group
launches two EC2 instances that register themselves with the control
plane as worker nodes.

[[a3--understand-the-application-code-while-the-cluster-provisions]]
==== A3 — Understand the Application Code (while the cluster provisions)

While the cluster creates, read through the application code. You don't
need to write this code — it's in the repository — but understanding it
is essential for the Kubernetes deployment steps.

Open the webapp code:

[source,bash]
----
cat app/webapp/app.py
----

The web app is a Flask application with four endpoints. `/submit`
accepts text via POST and pushes a task into a Redis list.
`/status/<task_id>` checks whether a specific task has been
processed. `/stats` returns the current queue depth and completed task
count. `/health` verifies the Redis connection is alive — this endpoint
becomes important when Kubernetes uses it for health checking.

Notice the environment variables at the top. `REDIS_HOST` and
`REDIS_PORT` default to `redis-service` and `6379`. The application
doesn't hardcode where Redis lives. Instead, Kubernetes DNS will resolve
`redis-service` to whatever pod is currently running Redis. This
decoupling is what makes the application portable across environments.

Now look at the worker:

[source,bash]
----
cat app/worker/worker.py
----

The worker runs an infinite loop. It calls `BRPOP` on the Redis task
queue, which blocks until a message is available (similar to SQS long
polling). When it gets a task, it computes word frequencies, stores the
result in Redis, and loops back to wait for the next task. If processing
fails, it re-queues the message. Each worker identifies itself using the
`HOSTNAME` environment variable, which Kubernetes sets to the pod name —
this lets you verify that work is distributed across multiple pods.

[[a4--examine-the-dockerfiles]]
==== A4 — Examine the Dockerfiles

[source,bash]
----
cat app/webapp/Dockerfile
----

Walk through each instruction:

`FROM python:3.12-slim` — Container images are layered. This base image
provides Python on a minimal Debian installation. The `slim` variant
excludes compilers and documentation, cutting the image from ~900 MB to
~150 MB. Every container you build starts from a base image like this.

`COPY requirements.txt .` followed by `RUN pip install` before
`COPY app.py .` — Docker caches each layer. If you change `app.py` but
not `requirements.txt`, Docker reuses the cached pip install layer and
only re-runs the copy of the changed file. This ordering is a build-time
optimization that matters in real development workflows where you
rebuild frequently.

`RUN useradd --create-home appuser` and `USER appuser` — Containers run
as root by default. If an attacker exploits a vulnerability in the Flask
app, running as root gives them full control of the container. Running
as a non-root user limits what an attacker can do.

`CMD ["gunicorn", ...]` — This is the process that runs when the
container starts. Gunicorn is a production WSGI server that handles
concurrent requests, replacing Flask's built-in development server.

[[a5--check-cluster-status]]
==== A5 — Check Cluster Status

By now the cluster should be close to ready. Check:

[source,bash]
----
# If the create-cluster script is still running in another terminal, check from here:
aws eks describe-cluster --name distlab --query 'cluster.status' --output text
----

When it returns `ACTIVE`, configure kubectl:

[source,bash]
----
aws eks update-kubeconfig --name distlab --region us-east-1
----

Then check that your nodes have joined:

[source,bash]
----
kubectl get nodes
----

You should see two nodes with status `Ready`. If they show `NotReady`,
wait a minute and check again — the nodes may still be initializing.

Explore what's already running on the cluster:

[source,bash]
----
kubectl get pods -A
----

This shows the system pods: `coredns` (DNS resolution for services),
`kube-proxy` (network rules on each node), and `aws-node` (the VPC CNI
plugin that assigns pod IP addresses from your VPC). These are the
infrastructure components that make Kubernetes networking work.

[[a6--build-and-push-container-images-15-minutes]]
==== A6 — Build and Push Container Images (15 minutes)

You need a machine with Docker to build images. CloudShell doesn't have
Docker, so you'll launch a small EC2 instance as a temporary build
server.

*Launch the build instance:*

[arabic]
. Open the EC2 console in a new browser tab.
. Launch an instance:
* *Name:* `distlab-build`
* *AMI:* Amazon Linux 2023
* *Instance type:* `t3.small`
* *Key pair:* `vockey`
* *IAM instance profile:* `LabInstanceProfile`
* *Storage:* 20 GB gp3
* *Security group:* Allow SSH (port 22) from your IP
* *User data* (under Advanced Details):

[source,bash]
----
#!/bin/bash
yum update -y
yum install -y docker git
systemctl start docker
systemctl enable docker
usermod -aG docker ec2-user
----

[arabic, start=3]
. Wait for the instance to reach `running` state and pass status checks
(~2 minutes).

*Connect and build:*

SSH into the instance from CloudShell (replace the IP):

[source,bash]
----
ssh -i ~/.ssh/labsuser.pem ec2-user@<BUILD_INSTANCE_PUBLIC_IP>
----

If you get a "Permission denied" error, the Docker group membership may
not have taken effect yet. Either wait another minute and reconnect, or
run `sudo usermod -aG docker ec2-user && newgrp docker`.

On the build instance:

[source,bash]
----
# Clone the lab repo
git clone <REPO_URL> k8s-lab
cd k8s-lab

# Run the build-and-push script
./scripts/build-and-push.sh
----

The script creates ECR repositories, builds both Docker images, and
pushes them. Watch the output — you'll see Docker execute each
Dockerfile instruction and cache layers.

When the script finishes, it prints the full ECR image URIs. Copy these
— you'll need them in the Kubernetes manifests. They look like:

....
123456789012.dkr.ecr.us-east-1.amazonaws.com/distlab-webapp:v1
123456789012.dkr.ecr.us-east-1.amazonaws.com/distlab-worker:v1
....

*Exit the build instance* and return to CloudShell:

[source,bash]
----
exit
----

You can stop or terminate the build instance now — it's no longer
needed. From the EC2 console, select the instance, choose Instance State
→ Terminate.

'''''

[[white_check_mark-checkpoint-end-of-session-a]]
==== ✅ Checkpoint: End of Session A

At this point you have:

* A running EKS cluster with two nodes
* Container images for the web app and worker in ECR
* kubectl configured in CloudShell

You can safely stop here. The cluster, nodes, and images persist between
sessions. When you resume, open CloudShell and verify:

[source,bash]
----
kubectl get nodes
----

If nodes show `NotReady`, wait 2–3 minutes for them to restart after a
new session begins.

'''''

=== Session B: Kubernetes Deployment and Experiments

[[b1--update-the-kubernetes-manifests-5-minutes]]
==== B1 — Update the Kubernetes Manifests (5 minutes)

The Kubernetes YAML files in the `k8s/` directory need your ECR image
URIs. Open each file and replace the placeholder:

[source,bash]
----
cd ~/k8s-lab

# Get your account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
ECR_BASE="$ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com"

# Replace placeholders in all manifests
sed -i "s|IMAGE_WEBAPP|$ECR_BASE/distlab-webapp:v1|g" k8s/webapp.yaml
sed -i "s|IMAGE_WORKER|$ECR_BASE/distlab-worker:v1|g" k8s/worker.yaml

# Verify the substitution worked
grep "image:" k8s/webapp.yaml k8s/worker.yaml
----

You should see your full ECR URIs in the output.

[[b2--deploy-redis-5-minutes]]
==== B2 — Deploy Redis (5 minutes)

Redis is the message broker and result store. Deploy it first because
the web app and workers depend on it.

[source,bash]
----
kubectl apply -f k8s/redis.yaml
----

Watch the pod come up:

[source,bash]
----
kubectl get pods -l app=redis -w
----

Wait until the STATUS column shows `Running` and READY shows `1/1`.
Press Ctrl{plus}C to stop watching.

Look at what was created:

[source,bash]
----
kubectl get all -l app=redis
kubectl get pvc
----

Three Kubernetes objects are at work here. The *Deployment* declares
"run 1 pod with the Redis 7 Alpine image." The Kubernetes controller
manager continuously compares this desired state against reality and
corrects any drift. If the pod dies, a new one appears.

The *Service* (`redis-service`) creates a stable DNS name. The web app
and workers connect to `redis-service:6379`. They never need to know the
pod's actual IP address, which changes every time a pod restarts.
Kubernetes DNS handles the resolution. This is service discovery — the
same problem that tools like Consul or etcd solve in non-Kubernetes
environments.

The *PersistentVolumeClaim* requests 1 GB of storage that outlives any
individual pod. Without it, Redis would lose all data every time its pod
restarted. Kubernetes binds this claim to an EBS volume in your AWS
account.

[[b3--deploy-the-web-app-10-minutes]]
==== B3 — Deploy the Web App (10 minutes)

[source,bash]
----
kubectl apply -f k8s/webapp.yaml
----

[source,bash]
----
kubectl get pods -l app=webapp -w
----

Wait for both replicas to show `Running` with `1/1` READY. This may take
60–90 seconds on the first deployment as the nodes pull the image from
ECR.

Check the Service:

[source,bash]
----
kubectl get svc webapp-service
----

The webapp service is type `LoadBalancer`. On EKS, this provisions an
actual AWS Elastic Load Balancer. The EXTERNAL-IP column will initially
show `<pending>` — it takes 2–3 minutes for the ELB to provision
and receive a DNS name.

Keep checking until you see a hostname:

[source,bash]
----
kubectl get svc webapp-service -w
----

Once the EXTERNAL-IP appears (something like
`a1b2c3-1234567890.us-east-1.elb.amazonaws.com`), save it:

[source,bash]
----
export APP_URL=$(kubectl get svc webapp-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "App URL: http://$APP_URL"
----

Test it:

[source,bash]
----
curl http://$APP_URL/health
curl http://$APP_URL/stats
----

The health endpoint should return
`{"redis": "connected", "status": "healthy"}`. If you get a
connection refused error, wait another minute for the ELB health checks
to pass and the target group to register the pods as healthy.

*Explain the readiness and liveness probes* (look at `k8s/webapp.yaml`):

The *readiness probe* hits `/health` every 10 seconds. Until it returns
HTTP 200, Kubernetes won't route traffic to that pod through the
Service. This prevents users from hitting a pod that hasn't finished
starting up or has lost its Redis connection.

The *liveness probe* also hits `/health`, but with a different purpose.
If it fails multiple consecutive times, Kubernetes kills the pod and
creates a new one. This handles the case where a process is running but
deadlocked or otherwise broken — it's still alive at the OS level but
not functional.

[[b4--deploy-the-workers-5-minutes]]
==== B4 — Deploy the Workers (5 minutes)

[source,bash]
----
kubectl apply -f k8s/worker.yaml
----

[source,bash]
----
kubectl get pods -l app=worker -w
----

Wait for both workers to be Running. Check that they connected to Redis:

[source,bash]
----
kubectl logs -l app=worker --tail=5
----

You should see "Waiting for tasks..." from each worker pod.

[[b5--run-the-pipeline-10-minutes]]
==== B5 — Run the Pipeline (10 minutes)

Submit text to the system and watch it flow through:

[source,bash]
----
# Submit 15 text samples
./scripts/submit-tasks.sh
----

In a second CloudShell tab, stream worker logs in real time:

[source,bash]
----
kubectl logs -l app=worker -f
----

You should see different worker pods (identifiable by their hostnames
like `worker-7d4f8b6c9-abc12` and `worker-7d4f8b6c9-xyz34`) processing
different tasks. This is distributed processing — Redis distributes work
and Kubernetes manages the workers.

Check the stats:

[source,bash]
----
curl http://$APP_URL/stats
----

The `completed_tasks` count should match the number of submissions,
and `queue_depth` should be 0 once workers have processed
everything.

Query a specific task result:

[source,bash]
----
# Replace with a task_id returned from the submit script
curl http://$APP_URL/status/<task_id>
----

The result includes the `worker_id` field, confirming which pod
processed the task.

[[b6--experiment-self-healing-15-minutes]]
==== B6 — Experiment: Self-Healing (15 minutes)

This is where orchestration earns its keep. Kill a worker and watch
Kubernetes replace it.

[arabic]
. List worker pods:
+
[source,bash]
----
kubectl get pods -l app=worker -o wide
----
+
Note the pod names, their ages, and which node each is running on.
. Delete a worker pod:
+
[source,bash]
----
kubectl delete pod <worker-pod-name>
----
. Immediately watch the response:
+
[source,bash]
----
kubectl get pods -l app=worker -w
----
+
You'll see the deleted pod enter `Terminating` status. Within seconds, a
new pod appears with status `Pending`, then `ContainerCreating`, then
`Running`. The Deployment controller detected that actual state (1
worker) diverged from desired state (2 workers) and launched a
replacement.
. Verify the new pod is processing tasks. Submit a few more:
+
[source,bash]
----
curl -s -X POST http://$APP_URL/submit \
  -H "Content-Type: application/json" \
  -d '{"text": "testing recovery after pod failure with some sample words to process"}'

kubectl logs -l app=worker --tail=5
----

*Now try killing the web app during active traffic:*

[arabic, start=5]
. In one terminal, run a loop that hits the health endpoint every
second:
+
[source,bash]
----
while true; do curl -s -o /dev/null -w "%{http_code} " http://$APP_URL/health; sleep 1; done
----
. In another terminal, delete one of the two webapp pods:
+
[source,bash]
----
kubectl delete pod $(kubectl get pods -l app=webapp -o jsonpath='{.items[0].metadata.name}')
----
. Watch the health check loop. Because there are two webapp replicas
behind the LoadBalancer, and Kubernetes only routes traffic to pods that
pass their readiness probe, you should see no (or very few) failed
requests. The remaining healthy pod continues serving while the
replacement starts up.

*Questions for the lab report:*

* How long did it take for the replacement pod to reach Running status?
What accounts for this time?
* Compare this to the Distributed Computing Lab (if completed): when an
EC2 worker was killed, in-flight SQS messages were invisible for 120
seconds before being retried. How does Kubernetes improve on this
recovery time?
* What would happen if you deleted both webapp pods simultaneously? How
could you configure the deployment to prevent this?

[[b7--experiment-scaling-15-minutes]]
==== B7 — Experiment: Scaling (15 minutes)

*Scale workers up:*

[source,bash]
----
kubectl scale deployment worker --replicas=4
kubectl get pods -l app=worker -w
----

Two new worker pods appear. Submit a large batch while four workers are
running:

[source,bash]
----
for i in $(seq 1 25); do
  curl -s -X POST http://$APP_URL/submit \
    -H "Content-Type: application/json" \
    -d "{\"text\": \"Batch test number $i with enough words to generate meaningful frequency data across multiple workers. Kubernetes orchestrates containers providing automated deployment scaling and management. The scheduler decides which node should run each new pod based on available resources.\"}" > /dev/null
done
echo "Submitted 25 tasks"
----

Watch the logs to see work distributed across all four pods:

[source,bash]
----
kubectl logs -l app=worker -f --prefix
----

The `--prefix` flag prepends the pod name to each log line, making it
easy to see which worker handles each task.

*Scale workers down:*

[source,bash]
----
kubectl scale deployment worker --replicas=1
----

Watch three of the four pods terminate. The remaining single worker
picks up all subsequent work.

[source,bash]
----
kubectl get pods -l app=worker -w
----

*Examine the node-level impact:*

[source,bash]
----
kubectl top pods
kubectl top nodes
----

(If `kubectl top` isn't available, you may need to wait a minute for the
metrics server to collect data, or check that the EKS metrics server
addon is active.)

*Questions for the lab report:*

* Was work evenly distributed across four workers? What factors affect
distribution when using Redis BRPOP?
* When scaling down from 4 to 1, how does Kubernetes decide which pods
to terminate?
* At what point would adding more worker pods stop improving throughput?
What becomes the bottleneck?
* The Learner Lab limits you to 9 concurrent EC2 instances and 32 vCPU
total. With two t3.small nodes (2 vCPU each), how many worker pods could
you realistically run? What would you need to change to run more?

[[b8--experiment-rolling-updates-15-minutes]]
==== B8 — Experiment: Rolling Updates (15 minutes)

Deploy a code change with zero downtime.

[arabic]
. Suppose you've built a `v2` image with a bug fix. For this experiment,
you can simulate a new version by retagging the existing image in ECR.
Run this from CloudShell:
+
[source,bash]
----
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
REGION="us-east-1"

# Pull the v1 manifest, retag as v2
MANIFEST=$(aws ecr batch-get-image --repository-name distlab-worker --image-ids imageTag=v1 \
  --query 'images[0].imageManifest' --output text --region $REGION)

aws ecr put-image --repository-name distlab-worker --image-tag v2 \
  --image-manifest "$MANIFEST" --region $REGION
----
. Update the deployment to use v2:
+
[source,bash]
----
ECR_BASE="$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com"
kubectl set image deployment/worker worker=$ECR_BASE/distlab-worker:v2
----
. Watch the rollout:
+
[source,bash]
----
kubectl rollout status deployment/worker
----
+
In another terminal:
+
[source,bash]
----
kubectl get pods -l app=worker -w
----
+
Kubernetes creates new pods with the v2 image and terminates old pods
one at a time. At no point are zero workers available — the rolling
update strategy ensures continuous availability.
. Check the rollout history:
+
[source,bash]
----
kubectl rollout history deployment/worker
----
. Roll back to v1:
+
[source,bash]
----
kubectl rollout undo deployment/worker
kubectl rollout status deployment/worker
----
+
The rollback is the same process in reverse. Kubernetes keeps track of
previous ReplicaSets so it can return to any prior version.

*Now deploy a broken image:*

[arabic, start=6]
. Update the worker to a nonexistent image tag:
+
[source,bash]
----
kubectl set image deployment/worker worker=$ECR_BASE/distlab-worker:v999
----
. Watch the failure:
+
[source,bash]
----
kubectl get pods -l app=worker -w
----
+
New pods show `ImagePullBackOff` or `ErrImagePull` — the image doesn't
exist in ECR. But notice that some old pods are still running. The
rolling update strategy didn't terminate all existing pods before
verifying the new ones worked.
. Diagnose the problem:
+
[source,bash]
----
kubectl describe pod $(kubectl get pods -l app=worker --field-selector=status.phase!=Running -o jsonpath='{.items[0].metadata.name}')
----
+
The Events section shows exactly what went wrong: "Failed to pull
image... not found."
. Fix it by rolling back:
+
[source,bash]
----
kubectl rollout undo deployment/worker
----

*Questions for the lab report:*

* During a rolling update with 2 replicas, how many pods exist
simultaneously? Why?
* What risks exist when both v1 and v2 pods run at the same time during
a transition? How would you handle a database schema change in this
scenario?
* How does the rolling update strategy protect you from deploying broken
code to production? What additional safeguards would you add?

[[b9--exploring-the-cluster-10-minutes]]
==== B9 — Exploring the Cluster (10 minutes)

Practice the diagnostic commands you'd use to troubleshoot a production
deployment.

[source,bash]
----
# Everything running in the default namespace
kubectl get all

# Detailed view of a specific pod — events, conditions, IP, node assignment
kubectl describe pod <any-pod-name>

# Events across the namespace, sorted by time
kubectl get events --sort-by='.lastTimestamp'

# Open a shell inside a running worker pod
kubectl exec -it $(kubectl get pods -l app=worker -o jsonpath='{.items[0].metadata.name}') -- /bin/bash
----

From inside the container:

[source,bash]
----
# Each container has its own hostname — the pod name
hostname

# Environment variables injected by Kubernetes
env | grep REDIS

# Verify the container can reach Redis through Kubernetes DNS
python3 -c "import redis; r = redis.Redis(host='redis-service'); print('Queue depth:', r.llen('task_queue'))"

# The filesystem is minimal — no vim, no ssh, no systemd
ls /usr/bin | wc -l

exit
----

This demonstrates the isolation model. Each container has its own
filesystem, hostname, process namespace, and network stack. It can reach
other services through Kubernetes DNS but is otherwise self-contained.
Compare this to an EC2 instance, which runs a full operating system with
hundreds of system services.

[[b10--cleanup-10-minutes]]
==== B10 — Cleanup (10 minutes)

*This step is critical for budget preservation.* A running EKS cluster
charges $0.10/hour for the control plane plus node instance costs, even
when you're not actively using it.

[source,bash]
----
cd ~/k8s-lab
./scripts/teardown.sh
----

The script deletes resources in the correct order:

[arabic]
. Kubernetes services (which trigger ELB deletion)
. Kubernetes deployments and other objects
. The EKS node group
. The EKS cluster
. ECR repositories

The script waits for each deletion to complete before moving to the next
step, since some resources depend on others. Full teardown takes 10–15
minutes.

If you plan to continue working in a later session and want to keep the
cluster running, you can skip this step — but be aware of the ongoing
cost.

'''''

=== Lab Deliverables

==== Screenshots

[arabic]
. `kubectl get pods` showing webapp, worker, and redis pods running
across your nodes
. Worker logs showing tasks distributed across multiple pods (include
pod names)
. The self-healing experiment: `kubectl get pods -w` output showing
automatic replacement after pod deletion
. The rolling update: pods transitioning between image versions
. `kubectl describe pod` from the broken-image debugging exercise, with
the Events section visible

==== Dockerfile Analysis

Annotate each line of the webapp Dockerfile (`app/webapp/Dockerfile`),
explaining what it does and why. Address these specific questions in
your annotations:

* Why are dependencies copied and installed before the application code?
* What does `--no-cache-dir` accomplish and why use it in a container?
* What security risk does the `USER appuser` line mitigate?
* Why use `gunicorn` instead of Flask's built-in server?

==== Comparison (400–600 words)

If you also completed the Distributed Computing Lab (SQS/EC2 version),
write a comparison addressing:

* How does each approach handle worker failure and task recovery?
* How does each approach handle scaling the worker pool?
* How does service discovery work in each architecture — how does the
producer find the queue, and how do workers find the data store?
* What does Kubernetes automate that required manual intervention or
custom scripting in the EC2 lab?
* Under what conditions would you choose the SQS/EC2 approach over
Kubernetes, and vice versa?

If you did not complete the Distributed Computing Lab, instead write
about the tradeoffs between running this application on EKS versus
deploying the same containers to ECS with Fargate (research Fargate's
model: no node management, per-task pricing, no kubectl).

==== Reflection Questions

[arabic]
. What is the difference between a container and a virtual machine? Why
does a container start faster?
. Explain the purpose of the three Kubernetes object types you deployed:
Deployment, Service, and PersistentVolumeClaim. What problem does each
solve that raw containers don't?
. The webapp has both a readiness probe and a liveness probe. What would
go wrong if you only had a liveness probe? What would go wrong if you
only had a readiness probe?
. The worker deployment has no Service. Why not? Under what
circumstances would you add one?
. If you needed to process 10,000 tasks per minute, what would be the
first bottleneck in this architecture? Walk through each component (ELB
→ webapp → Redis → worker) and identify where throughput would degrade
first.

'''''

=== Instructor Notes

==== Timing

EKS cluster creation takes 12–18 minutes. The lab is structured to use
this time for code review and Dockerfile analysis, but if students are
fast readers, have them explore the AWS EKS console to see the cluster
provisioning in real time.

The LoadBalancer service takes 2–3 minutes to provision an ELB. Students
sometimes think the deployment failed when they see `<pending>`
— reassure them that this is normal AWS ELB provisioning.

==== Common Issues

*"Could not find EKS role"* — The create-cluster script looks for roles
containing "Eks" (case-insensitive) in the account. If the Learner Lab
environment uses a different naming convention, students may need to run
`aws iam list-roles` and identify the correct role manually. Update the
script's grep pattern if needed.

*ECR push fails with "AccessDenied"* — The LabRole has read-only ECR
access. The build-and-push script creates ECR repositories using the EC2
instance credentials, which may lack `ecr:CreateRepository` permissions.
If this happens, have students create the repositories from the ECR
console (which uses console-level credentials that do have write
access), then re-run the script.

*Nodes show `NotReady` for more than 5 minutes* — Check that the node
group's IAM role has the required EKS policies. Run
`aws eks describe-nodegroup --cluster-name distlab --nodegroup-name distlab-nodes`
and verify the role ARN. The nodes need `AmazonEKSWorkerNodePolicy`,
`AmazonEKS_CNI_Policy`, and
`AmazonEC2ContainerRegistryReadOnly`.

*`kubectl` returns "error: exec plugin is configured to use API
version"* — Run
`aws eks update-kubeconfig --name distlab --region us-east-1` to
regenerate the kubeconfig. This sometimes happens when the AWS CLI
version changes between CloudShell sessions.

==== Extension Activities

* Write a HorizontalPodAutoscaler that scales workers based on CPU
utilization. Submit a large burst of tasks and observe the autoscaler
add and remove pods.
* Create a ConfigMap to externalize the Redis connection parameters
rather than hardcoding them in the deployment manifests.
* Implement a NetworkPolicy that restricts worker pods to communicating
only with the Redis service.
* Build a multi-stage Dockerfile for the webapp that compiles
dependencies in a builder stage and copies only runtime artifacts to a
minimal base image. Compare the image sizes.
* Deploy a second version of the worker with different processing logic
(e.g., character frequency instead of word frequency) and use Kubernetes
labels and selectors to run both versions simultaneously.

==== Concept Mapping

[cols=",",options="header",]
|===
|Lab Component |Concept
|Dockerfile, `docker build` |Image layering, build caching, immutable
artifacts

|ECR push/pull |Container registry, image distribution

|EKS cluster |Managed control plane, node registration

|Deployment |Declarative desired state, reconciliation loop

|Service (ClusterIP) |Service discovery, stable networking

|Service (LoadBalancer) |External traffic ingress, cloud integration

|PersistentVolumeClaim |Stateful storage in ephemeral environments

|Readiness/liveness probes |Health checking, traffic management

|Pod deletion experiment |Self-healing, desired-state reconciliation

|`kubectl scale` |Horizontal scaling

|Rolling update/rollback |Zero-downtime deployment, version management

|`kubectl exec` |Container isolation, namespace boundaries

|Broken image debugging |Failure diagnosis, rollout safety
|===
